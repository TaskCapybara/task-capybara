{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "# Prompt Notebook with Chat - Prompt Lab Notebook v1.1.0\n",
    "This notebook contains steps and code to demonstrate inferencing of prompts\n",
    "generated in Prompt Lab in watsonx.ai with a chat format. It introduces Python API commands\n",
    "for authentication using API key and prompt inferencing using WML API.\n",
    "\n",
    "**Note:** Notebook code generated using Prompt Lab will execute successfully.\n",
    "If code is modified or reordered, there is no guarantee it will successfully execute.\n",
    "For details, see: <a href=\"/docs/content/wsj/analyze-data/fm-prompt-save.html?context=wx\" target=\"_blank\">Saving your work in Prompt Lab as a notebook.</a>\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.10.\n",
    "\n",
    "## Notebook goals\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "* Defining a Python function for obtaining credentials from the IBM Cloud personal API key\n",
    "* Defining parameters of the Model object\n",
    "* Using the Model object to generate response using the defined model id, parameters and the prompt input\n",
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## watsonx API connection\n",
    "This cell defines the credentials required to work with watsonx API for Foundation\n",
    "Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud personal API key. For details, see\n",
    "<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def get_credentials():\n",
    "\treturn {\n",
    "\t\t\"url\" : \"https://us-south.ml.cloud.ibm.com\",\n",
    "\t\t\"apikey\" : \"Bql91PNKGI6BsmCET-yb9KO_ozMr4bSi5Gc33067BTtr\"\n",
    "#         getpass.getpass(\"Please enter your api key (hit enter): \")\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing\n",
    "This cell demonstrated how we can use the model object as well as the created access token\n",
    "to pair it with parameters and input string to obtain\n",
    "the response from the the selected foundation model.\n",
    "\n",
    "## Defining the model id\n",
    "We need to specify model id that will be used for inferencing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/llama-3-405b-instruct\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model parameters\n",
    "We need to provide a set of model parameters that will influence the\n",
    "result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"decoding_method\": \"greedy\",\n",
    "    \"max_new_tokens\": 900,\n",
    "    \"repetition_penalty\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the project id or space id\n",
    "The API requires project id or space id that provides the context for the call. We will obtain\n",
    "the id from the project or space in which this notebook runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_id = os.getenv(\"PROJECT_ID\")\n",
    "project_id = 'f64f5842-5256-40e9-ad42-478af13a96fc'\n",
    "# space_id = os.getenv(\"SPACE_ID\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model object\n",
    "We need to define the Model object using the properties we defined so far:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "\n",
    "model = Model(\n",
    "\tmodel_id = model_id,\n",
    "\tparams = parameters,\n",
    "\tcredentials = get_credentials(),\n",
    "\tproject_id = project_id,\n",
    "\tspace_id = None\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inferencing input for chat\n",
    "Foundation models supporting chat accept a system prompt that instructs the model on how to conduct the dialog. They also accept previous questions and answers to give additional context when inferencing. Each model has it's own string format for constructing the input.\n",
    "\n",
    "Let us provide the input we got from the Prompt Lab and format it for the selected model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful, friendly, and professional team management assistant designed to gather daily progress updates from team members. \n",
    "Your goal is to ask the right questions to get detailed updates on task progress, blockers, and timelines. \n",
    "You should ask one question at a time. \n",
    "You should also maintain a conversational tone, encourage clarity, and offer help when needed. \n",
    "Always summarize the information clearly.\n",
    "\"\"\"\n",
    "# prompt_input = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "# You are a helpful, friendly, and professional team management assistant designed to gather daily progress updates from team members. Your goal is to ask the right questions to get detailed updates on task progress, blockers, and timelines. You should ask one question at a time. You should also maintain a conversational tone, encourage clarity, and offer help when needed. Always summarize the information clearly.\n",
    "\n",
    "# Key actions:\n",
    "# 1. Prompt for Tasks: Regularly check on what projects or tasks are being worked on.\n",
    "# 2. Monitor Status: Ask for updates, ensuring clarity on whether tasks are \\\"not started\\\", \\\"in progress\\\", \\\"developed\\\", \\\"in testing\\\", \\\"tested\\\", or \\\"closed.\\\"\n",
    "# 3. Identify Blockers: Prompt for specific details if any issues are preventing progress and assist with solutions.\n",
    "# 4. Clarify Deadlines: Confirm important deadlines to keep the team on track, such as \\\"tomorrow 6pm\\\" or \\\"by next Friday morning.\\\"\n",
    "\n",
    "# When team members give vague or incomplete responses, you prompt for more detailed information and request clarification where needed. Summarize discussions clearly to keep everyone aligned.\n",
    "# \"\"\"\n",
    "current_prompt_input = prompt_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "Let us now use the defined Model object, pair it with the input, and generate the response to your question:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hi\n",
      "AI: Hello! How's your day going so far? I'm here to help gather your daily progress update. Can you start by telling me what tasks you've been working on today?\n",
      "User: build gen ai\n",
      "AI: So you're working on building a general AI. That's a significant project. Can you tell me what specific aspect of the general AI you're focusing on today, such as data preparation, model architecture, or training?\n",
      "User: save gen ai response to model 2\n",
      "AI: So it sounds like you're working on saving the responses from the general AI to a second model, Model 2. That's a great step in building a more comprehensive AI system.\n",
      "\n",
      "Can you tell me what's the current status of this task? Are you making good progress, or are you facing any challenges or blockers that I can help with?\n",
      "User: no\n",
      "AI: So, it sounds like you're not facing any specific challenges or blockers with saving the general AI responses to Model 2, which is great.\n",
      "\n",
      "Next question: Do you have an estimated timeline for completing this task, or is there a specific deadline you're working towards?\n"
     ]
    }
   ],
   "source": [
    "with open(\"conversation_log.txt\", \"a\") as file:\n",
    "    while True:\n",
    "        user_input = input(\"User Input: \")\n",
    "\n",
    "        formatted_question = f\"\"\"<|begin_of_text|><|eot_id|><|start_header_id|>user<|end_header_id|>{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "        print(f\"User: {user_input}\")\n",
    "\n",
    "        prompt = f\"\"\"{current_prompt_input}{formatted_question}\"\"\"\n",
    "        generated_response = model.generate_text(prompt=prompt)\n",
    "        print(f\"AI: {generated_response}\")\n",
    "        \n",
    "        current_prompt_input += f\"\"\"<|eot_id|><|start_header_id|>user<|end_header_id|>{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>{generated_response}\"\"\"\n",
    "\n",
    "        file.write(f\"User: {user_input}\\nAI: {generated_response}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ibm-watsonx/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f716cac889840778ea2c240e605070c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331a6b2cb1174764b10053b70caf657b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b54840d86664c1489bce480bdb63a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c47d51b8b741a497a00b0822d7b97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4699c373a7fc402c8dd9fbbb4d24d926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9cdabd41ba4d88a7f0f54c958e1360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d948e3fe39845f896d426369cfada64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d391dd07b84e6dbab3e46fd2ccd5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a13320f022445a94264991d1f08911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884377a9b4614b4fa81673170cee6bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ibm-watsonx/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74dc021340e449398411257f66a14432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.91045800e-02 -3.26819867e-02  5.10530621e-02  3.31608094e-02\n",
      "  6.84638247e-02 -5.06282132e-03 -6.17100634e-02  4.00642417e-02\n",
      " -6.43311441e-02 -1.24266539e-02 -4.63233739e-02 -8.79119188e-02\n",
      " -5.70283569e-02  8.31481721e-03 -1.99319553e-02  4.73637395e-02\n",
      "  2.22926941e-02 -1.14441000e-01 -7.55249485e-02 -3.83362398e-02\n",
      "  4.27678488e-02 -2.35262644e-02  5.37086278e-02  2.96468064e-02\n",
      " -5.08574434e-02  8.70735496e-02  4.87340055e-02 -3.06332149e-02\n",
      " -2.35194080e-02 -2.19384767e-02 -1.03021562e-02 -6.59518391e-02\n",
      "  4.68445420e-02  2.12450931e-03 -2.23529786e-02  2.93348059e-02\n",
      " -3.85857485e-02  1.96991023e-02  6.99891374e-02 -8.16947743e-02\n",
      " -1.50374305e-02 -4.49657068e-02 -6.49809651e-03  1.98469665e-02\n",
      "  1.09718651e-01  1.16633987e-02 -1.34847341e-02 -5.16464375e-02\n",
      "  3.32548171e-02  3.32077667e-02 -8.04681033e-02 -2.61694882e-02\n",
      "  2.11472530e-02  9.46835615e-03  4.67954688e-02  1.07905403e-01\n",
      "  4.03963402e-02 -5.13790548e-02 -8.22452642e-03 -3.58789563e-02\n",
      "  1.06613599e-02 -1.53800016e-02 -3.19318101e-02 -5.44004180e-02\n",
      "  3.43014486e-02  5.46508376e-03 -4.75787483e-02 -1.41006801e-02\n",
      "  1.33309513e-02 -3.10587324e-03 -3.59373279e-02  7.36548528e-02\n",
      " -4.66349721e-02 -2.57847756e-02  2.67985426e-02 -2.95225959e-02\n",
      "  6.55912934e-03  7.19760433e-02  6.06299229e-02 -7.42745996e-02\n",
      " -1.49860010e-02 -1.19866189e-02 -1.51811140e-02  1.01702530e-02\n",
      " -3.54307331e-02 -2.32646279e-02  5.96412718e-02  6.86466172e-02\n",
      "  6.75863698e-02 -2.13541370e-02  5.08157536e-02  4.39818343e-03\n",
      "  1.16838574e-01  4.42045517e-02  9.73437503e-02  7.94053748e-02\n",
      "  3.24854441e-03 -2.58038417e-02 -2.77806595e-02  6.04826845e-02\n",
      "  4.13841568e-02  6.11374527e-03  1.62193496e-02 -5.36119044e-02\n",
      "  2.22844146e-02  3.00729275e-02  1.40013965e-02 -7.85487238e-03\n",
      " -7.22592324e-02 -4.70611155e-02  1.77917536e-02  2.03810483e-02\n",
      "  2.54006069e-02 -1.31269600e-02  7.08506405e-02  2.36957017e-02\n",
      " -1.47194043e-02  7.61772171e-02  5.42582460e-02  9.54900533e-02\n",
      "  5.02154194e-02  2.68675424e-02  5.41871078e-02  5.49638681e-02\n",
      "  1.24638593e-02 -4.54917084e-03 -1.17592672e-02  5.02802317e-33\n",
      "  7.89472181e-03  4.52150516e-02  6.35133609e-02  7.41813630e-02\n",
      "  8.04154016e-03 -1.69928521e-02  9.45181865e-03  5.57402819e-02\n",
      "  9.61820688e-03 -6.37953654e-02 -8.55555162e-02 -4.09277193e-02\n",
      " -8.23066309e-02  2.38043536e-02 -5.12997899e-03 -6.99625313e-02\n",
      "  8.91517662e-03  1.84956659e-03  3.63669824e-03  7.21561015e-02\n",
      "  8.72694626e-02 -9.05123204e-02  6.35549193e-03  1.91714484e-02\n",
      "  1.35500744e-01 -2.07579155e-02  1.02875412e-01 -8.02840590e-02\n",
      " -7.29834065e-02 -1.12377629e-02 -5.34619614e-02 -3.44422348e-02\n",
      " -5.42884693e-02  2.56947968e-02 -3.97911519e-02  5.97946253e-03\n",
      "  3.61928367e-03 -2.83999275e-02 -2.64781658e-02 -3.98145691e-02\n",
      "  1.00501804e-02  4.70063575e-02 -5.20154322e-03 -4.68700379e-02\n",
      " -7.76880234e-02 -1.10127397e-01  1.90635342e-02 -8.09202790e-02\n",
      " -2.63115689e-02  3.43726166e-02 -1.27078053e-02  8.34118109e-03\n",
      " -4.41155843e-02 -1.12932190e-01 -2.52592154e-02 -5.86233810e-02\n",
      " -3.83117497e-02 -2.07386557e-02  1.35956472e-02 -2.35182070e-03\n",
      "  9.09398571e-02 -5.67506216e-02 -5.04887663e-02  5.39925061e-02\n",
      " -5.84493764e-02  7.40671381e-02 -4.38611098e-02  7.89938644e-02\n",
      "  1.24364644e-01  1.23135522e-02  1.13432910e-02 -6.05250932e-02\n",
      "  3.81681547e-02 -1.21551268e-02  3.40959877e-02  9.28262528e-03\n",
      " -4.77997772e-03 -9.94849876e-02 -3.44370380e-02  1.97547139e-03\n",
      " -2.49282606e-02  3.19858007e-02 -1.16886109e-01 -4.98924442e-02\n",
      "  4.03956547e-02 -2.45376043e-02 -1.04369726e-02  3.81944478e-02\n",
      " -5.28708147e-03 -7.12600909e-03 -9.87605006e-02  4.52621728e-02\n",
      " -4.30652425e-02  7.48747736e-02 -1.95234101e-02 -4.71854379e-33\n",
      "  5.35038263e-02  3.87836568e-04 -4.24412638e-02 -4.05506939e-02\n",
      "  7.49347135e-02 -6.62711114e-02 -6.18034638e-02  1.31111387e-02\n",
      "  8.03667214e-03  1.41764292e-02 -5.47755100e-02  1.80189207e-03\n",
      "  1.14102988e-02  3.74672934e-03 -6.62563369e-02 -4.92546447e-02\n",
      "  3.52286249e-02 -4.88795713e-02  1.37856221e-02  2.71794703e-02\n",
      " -6.04483932e-02  8.13158154e-02 -1.07437611e-01 -6.44860268e-02\n",
      "  2.78884824e-02  5.02216667e-02 -1.33898500e-02  9.99169946e-02\n",
      "  3.86556163e-02 -2.81830225e-02 -3.29496562e-02 -6.76884130e-02\n",
      " -3.00243106e-02 -1.99540816e-02  2.58762166e-02  7.70976692e-02\n",
      "  5.04399166e-02 -6.47020862e-02 -3.19284610e-02  8.98502767e-02\n",
      "  8.93412605e-02 -1.53490426e-02 -3.91996913e-02  2.91322432e-02\n",
      " -4.89322841e-03  7.96341971e-02  7.30662514e-03  3.89940036e-03\n",
      " -1.03154734e-01 -2.05512438e-02 -1.86394732e-02  2.88298093e-02\n",
      " -3.13927047e-02 -3.37067582e-02  4.10969107e-04 -2.41716523e-02\n",
      "  3.53884920e-02 -8.32659081e-02 -1.82092525e-02  1.18362000e-02\n",
      " -9.78695303e-02  1.99853964e-02  7.87713081e-02 -4.62721027e-02\n",
      "  1.86330006e-02 -3.04362755e-02  9.98375285e-03  1.29817827e-02\n",
      "  7.36905728e-03 -6.62900554e-03 -2.98062898e-02  2.42472757e-02\n",
      " -2.14921720e-02  4.04538810e-02  1.61438268e-02 -2.82978769e-02\n",
      " -4.59783785e-02 -4.95535806e-02  5.70852980e-02 -2.47887876e-02\n",
      " -4.88851368e-02 -1.54378405e-03  4.45694430e-03  3.17760333e-02\n",
      " -2.12516431e-02  6.39394997e-03  3.20819952e-02  8.33711177e-02\n",
      "  2.40998641e-02  2.75973901e-02 -3.19191702e-02 -1.50287962e-02\n",
      " -7.99417868e-02  9.60840434e-02 -4.84510045e-03 -6.12420834e-08\n",
      "  8.33238289e-03  7.42350295e-02  2.16639526e-02  3.39108445e-02\n",
      "  3.78232896e-02  1.20811183e-02 -8.33433121e-02 -3.42194661e-02\n",
      "  4.02069241e-02 -8.27918202e-02  7.15066940e-02 -7.10368110e-03\n",
      " -6.45217532e-03 -9.92083224e-04  3.66364159e-02 -9.02939029e-03\n",
      "  5.75145371e-02  3.03379335e-02  1.89030431e-02 -1.38627350e-01\n",
      "  1.25901476e-01 -1.41598852e-02 -3.32586020e-02 -2.57428885e-02\n",
      "  3.08657233e-02 -4.70845588e-02  2.57828948e-03  5.48326224e-02\n",
      " -7.52478614e-02  2.68148854e-02  1.22375842e-02  3.25923897e-02\n",
      "  6.46061227e-02 -9.67993028e-03  5.40378653e-02 -3.96662988e-02\n",
      "  4.14058119e-02 -3.52680385e-02  4.73788716e-02 -3.88429612e-02\n",
      "  8.62815157e-02  8.56079683e-02 -1.37845865e-02 -4.30787541e-03\n",
      "  6.34876937e-02 -5.67761920e-02 -5.64120784e-02 -1.32468462e-01\n",
      "  2.69475058e-02 -2.24456247e-02 -2.60010362e-02 -6.86538070e-02\n",
      "  7.60427490e-02  1.63916156e-01  4.92756255e-02  2.98083816e-02\n",
      "  1.87431462e-02 -6.34261817e-02  3.54346149e-02  4.05094004e-04\n",
      "  1.08501218e-01  2.15238035e-02 -1.16809539e-01  6.60808310e-02]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "with open(\"conversation_log.txt\", \"r\") as file:\n",
    "    conversation_entry = file.read()\n",
    "embedding = model.encode(conversation_entry)\n",
    "print(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referring on https://milvus.io/docs/quickstart.md\n",
    "from pymilvus import MilvusClient\n",
    "\n",
    "client = MilvusClient(uri=\"./conversation_history.db\")\n",
    "\n",
    "collection_name = \"my_rag_collection\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.has_collection(collection_name=\"my_rag_collection\"):\n",
    "    client.drop_collection(collection_name=\"my_rag_collection\")\n",
    "client.create_collection(\n",
    "    collection_name=\"my_rag_collection\",\n",
    "    dimension=768, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id_lead = \"ibm/granite-20b-multilingual\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "You successfully completed this notebook! You learned how to use\n",
    "watsonx.ai inferencing SDK to generate response from the foundation model\n",
    "based on the provided input, model id and model parameters. Check out the\n",
    "official watsonx.ai site for more samples, tutorials, documentation, how-tos, and blog posts.\n",
    "\n",
    "<a id=\"copyrights\"></a>\n",
    "### Copyrights\n",
    "\n",
    "Licensed Materials - Copyright © 2023 IBM. This notebook and its source code are released under the terms of the ILAN License.\n",
    "Use, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n",
    "\n",
    "**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for Watson Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n",
    "\n",
    "By downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
